{"cells":[{"cell_type":"markdown","id":"62e2ce48","metadata":{"id":"62e2ce48"},"source":["# Summary\n","To compare results between NB model trained on without vs with stopwords removed."]},{"cell_type":"markdown","id":"b497009b","metadata":{"id":"b497009b"},"source":["# 1. Importing necessary libraries and reading the data from a CSV file.\n"]},{"cell_type":"markdown","id":"7af58e16","metadata":{"id":"7af58e16"},"source":["Importing all relevant libraries"]},{"cell_type":"code","execution_count":null,"id":"e337fec0","metadata":{"id":"e337fec0"},"outputs":[],"source":["# Installations\n","# !pip install contractions"]},{"cell_type":"code","execution_count":null,"id":"8f5eca85","metadata":{"id":"8f5eca85"},"outputs":[],"source":["#general libraries\n","import pandas as pd\n","import numpy as np\n","import gensim\n","from sklearn.model_selection import train_test_split\n","\n","#for Vader\n","import nltk\n","# nltk.download('vader_lexicon')\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","sent_analyzer = SentimentIntensityAnalyzer()\n","\n","#for decontract\n","import re\n","import contractions\n","\n","#confusion matrix\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n","from sklearn.metrics import confusion_matrix, classification_report"]},{"cell_type":"code","execution_count":null,"id":"e94c0cfb","metadata":{"id":"e94c0cfb","outputId":"78c91844-3726-4140-b266-d659f59335aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to C:\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["pd.options.mode.chained_assignment = None\n","nltk.download('stopwords')\n","english_stop_list = nltk.corpus.stopwords.words('english')\n","indonesian_stop_list = nltk.corpus.stopwords.words('indonesian')\n","stop_list = english_stop_list.copy()\n","stop_list.extend(indonesian_stop_list)"]},{"cell_type":"markdown","id":"6947666c","metadata":{"id":"6947666c"},"source":["# 2. Data preprocessing, including expanding contractions and assigning gold truth labels based on review scores."]},{"cell_type":"code","execution_count":null,"id":"0380e77b","metadata":{"id":"0380e77b","scrolled":true},"outputs":[],"source":["df = pd.read_csv('cleaned_grab_playstore_reviews.csv')\n","\n","df = df.drop('Date', axis=1)"]},{"cell_type":"code","execution_count":null,"id":"1035ba42","metadata":{"id":"1035ba42"},"outputs":[],"source":["#obtain list of polarity based on scores\n","score_polarity = []\n","for i in range(len(df)):\n","    if df['Score'][i] == 3:\n","        score_polarity.append('neutral')\n","    elif df['Score'][i] < 3:\n","        score_polarity.append('negative')\n","    else:\n","        score_polarity.append('positive')\n","\n","# Expand contractions within the reviews which include those like cant, dont\n","def decontract_reviews(text):\n","    return contractions.fix(text)\n","\n","df['Review_SA_Processed'] = df['Review'].apply(decontract_reviews)"]},{"cell_type":"code","execution_count":null,"id":"6bc8f29c","metadata":{"id":"6bc8f29c","outputId":"fa7164fc-2d68-4023-c3ed-2f681e1bb23b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review_SA_Processed</th>\n","      <th>Score</th>\n","      <th>gold_truth</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>update poor performance taking minutes search ...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>bad app</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>friendly helpful nice well maintained vehicle ...</td>\n","      <td>5</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>remove grab app looking food knew car number w...</td>\n","      <td>2</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>embarassing singapore waiting time reduced min...</td>\n","      <td>2</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9802</th>\n","      <td>good heavens future taxi trip arrangements tha...</td>\n","      <td>5</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9803</th>\n","      <td>apa punya babi customer service baru nak typin...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>9804</th>\n","      <td>grab app helpful service fast amazing delivere...</td>\n","      <td>5</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9805</th>\n","      <td>trying book ride broad daylight specific spent...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>9806</th>\n","      <td>cashing easy cannot use credit pay cashless tr...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9807 rows × 3 columns</p>\n","</div>"],"text/plain":["                                    Review_SA_Processed  Score gold_truth\n","0     update poor performance taking minutes search ...      1   negative\n","1                                               bad app      1   negative\n","2     friendly helpful nice well maintained vehicle ...      5   positive\n","3     remove grab app looking food knew car number w...      2   negative\n","4     embarassing singapore waiting time reduced min...      2   negative\n","...                                                 ...    ...        ...\n","9802  good heavens future taxi trip arrangements tha...      5   positive\n","9803  apa punya babi customer service baru nak typin...      1   negative\n","9804  grab app helpful service fast amazing delivere...      5   positive\n","9805  trying book ride broad daylight specific spent...      1   negative\n","9806  cashing easy cannot use credit pay cashless tr...      1   negative\n","\n","[9807 rows x 3 columns]"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["df = df.assign(gold_truth=score_polarity)\n","df = df.reindex(columns=['Review_SA_Processed', 'Score', 'gold_truth'])\n","df\n","# df.loc[df['gold_truth'] == 'neutral']\n","# df[(df['Review_SA_Processed'].str.contains('good')) & (df['Score'].isin([1, 2]))]\n"]},{"cell_type":"markdown","id":"d07a152a","metadata":{"id":"d07a152a"},"source":["# 3.1 Training a Naive Bayes Classifier (Probablistic) with Stopwords"]},{"cell_type":"code","execution_count":null,"id":"8567e56a","metadata":{"id":"8567e56a"},"outputs":[],"source":["#clone original df\n","df_copy = df.copy(deep=True)\n","\n","#make x and y for train_test split\n","y = df_copy.pop('gold_truth')\n","\n","X = df_copy"]},{"cell_type":"code","execution_count":null,"id":"1d2b58c3","metadata":{"id":"1d2b58c3","outputId":"906284f3-0d16-49e9-97c8-1c57770f0d8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished preparing the training data.\n","Finished training the classifier.\n","Finished preparing the test data.\n"]}],"source":["#train-test split the data, where corpus = X and labels = y\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n","\n","# We use the following list to store the sentences, where each sentence itself is a list of words.\n","X_train_corpus = []\n","\n","for i in range(len(X_train)):\n","    # Tokenize the text.\n","    sent = nltk.word_tokenize(X_train['Review_SA_Processed'].iloc[i])\n","\n","    # Store the sentence into the corpus.\n","    X_train_corpus.append(sent)\n","\n","# Create a dictionary from the corpus.\n","dictionary = gensim.corpora.Dictionary(X_train_corpus)\n","\n","# Store the labeled training data in the following list.\n","labeled_training_data = []\n","\n","# Going through the two lists in parallel to create the labeled data set.\n","for (l, s) in zip(y_train, X_train_corpus):\n","\n","    # Convert the original sentence into a vector.\n","    vector = dictionary.doc2bow(s)\n","\n","    # Create a dict object to store the document vector (in order to use NLTK's classifier later)\n","    sent_as_dict = {id:1 for (id, tf) in vector}\n","\n","    # Add the labeled sentence to the labeled data set.\n","    labeled_training_data.append((sent_as_dict, l))\n","\n","print('Finished preparing the training data.')\n","\n","# Training Naive Bayes classifier.\n","classifierWithStopwords = nltk.NaiveBayesClassifier.train(labeled_training_data)\n","\n","print('Finished training the classifier.')\n","\n","# Store the labeled test data in the following list.\n","labeled_test_data = []\n","\n","X_test_corpus = []\n","\n","for i in range(len(X_test)):\n","    # Tokenize the text.\n","    sent = nltk.word_tokenize(X_test['Review_SA_Processed'].iloc[i])\n","\n","    # Store the sentence into the corpus.\n","    X_test_corpus.append(sent)\n","\n","# Going through the two lists in parallel to create the labeled data set.\n","for (l, s) in zip(y_test, X_test_corpus):\n","\n","    # Convert the original sentence into a vector.\n","    vector = dictionary.doc2bow(s)\n","\n","    # Create a dict object to store the document vector (in order to use NLTK's classifier later)\n","    sent_as_dict = {id:1 for (id, tf) in vector}\n","\n","    # Add the labeled sentence to the labeled data set.\n","    labeled_test_data.append((sent_as_dict, l))\n","\n","print('Finished preparing the test data.')"]},{"cell_type":"code","execution_count":null,"id":"82b6fba4","metadata":{"id":"82b6fba4","outputId":"436b57f1-422f-4545-890e-398668a2294e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Naive Bayes with Stopwords\n","Confusion Matrix:\n"," [[ 902  226   26]\n"," [  99   34   16]\n"," [ 317  165 1158]]\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","    negative       0.68      0.78      0.73      1154\n","     neutral       0.08      0.23      0.12       149\n","    positive       0.96      0.71      0.82      1640\n","\n","    accuracy                           0.71      2943\n","   macro avg       0.58      0.57      0.55      2943\n","weighted avg       0.81      0.71      0.75      2943\n","\n"]}],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Convert the labeled test data to a list of features and labels\n","test_set = [(features, label) for (features, label) in labeled_test_data]\n","\n","# Create a list of predicted labels and a list of gold truth labels for the test set\n","y_pred = [classifierWithStopwords.classify(features) for (features, label) in test_set]\n","y_true = [label for (features, label) in test_set]\n","\n","# Generate confusion matrix\n","nb_conf_mat = confusion_matrix(y_true, y_pred)\n","\n","# Generate classification report\n","nb_class_report = classification_report(y_true, y_pred, output_dict=True)\n","\n","# Confusion Matrix and Classification Report with Stopwords\n","\n","cm_with_stopwords = nb_conf_mat\n","cr_with_stopwords = classification_report(y_true, y_pred)\n","cr_with_stopwords_dict = classification_report(y_true, y_pred, output_dict=True)\n","\n","# Print results\n","print(\"Naive Bayes with Stopwords\")\n","print(\"Confusion Matrix:\\n\", cm_with_stopwords)\n","print(\"\\nClassification Report:\\n\", cr_with_stopwords)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"176cdd6d","metadata":{"id":"176cdd6d","outputId":"aa9d82b2-dc07-4b22-d240-dd578b62d80c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: Naive Bayes\n","Custom Input |  Predicted Sentiment\n","Good driver service! | positive\n","Bad driver service.. | negative\n","this is an app for driver and rider  | neutral\n","i like this app!! | neutral\n"]}],"source":["# Define the function to preprocess the sentences\n","def preprocess_sentence(sentence):\n","    # Convert to lowercase\n","    sentence = sentence.lower()\n","\n","    # Expand contractions\n","    sentence = decontract_reviews(sentence)\n","\n","    # Remove punctuation\n","    sentence = re.sub('[^\\w\\s]', '', sentence)\n","\n","    # Tokenize the sentence\n","    tokens = nltk.word_tokenize(sentence)\n","\n","    return tokens\n","\n","# List of sentences to test\n","sentences = [\"Good driver service!\", \"Bad driver service..\", \"this is an app for driver and rider \", \"i like this app!!\"]\n","\n","print(\"Model: Naive Bayes\")\n","print(\"Custom Input\", \"| \", \"Predicted Sentiment\")\n","\n","# Iterate over the list of sentences and predict the sentiment for each\n","for sentence in sentences:\n","    # Preprocess the sentence\n","    processed_sentence = preprocess_sentence(sentence)\n","\n","    # Convert the tokenized sentence to a vector\n","    vector = dictionary.doc2bow(processed_sentence)\n","\n","    # Create a dictionary object to store the document vector (in order to use NLTK's classifier)\n","    sent_as_dict = {id: 1 for (id, tf) in vector}\n","\n","    # Use the Naive Bayes classifier to predict the sentiment of the sentence\n","    predicted_sentiment = classifierWithStopwords.classify(sent_as_dict)\n","\n","    # Print the predicted sentiment\n","    print(sentence, \"|\", predicted_sentiment)\n"]},{"cell_type":"code","execution_count":null,"id":"27b0b1a3","metadata":{"id":"27b0b1a3","outputId":"d8d23278-c325-4ae5-f62c-97864e604169"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review_SA_Processed</th>\n","      <th>gold_truth</th>\n","      <th>Naive_Bayes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>673</th>\n","      <td>good driver nice clean car</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3101</th>\n","      <td>great service malaysia</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9508</th>\n","      <td>ovo tidak terkoneksi</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>6489</th>\n","      <td>give option rider wants accept nearer driver t...</td>\n","      <td>neutral</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4001</th>\n","      <td>good</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                    Review_SA_Processed gold_truth Naive_Bayes\n","673                          good driver nice clean car   positive    positive\n","3101                             great service malaysia   positive    positive\n","9508                               ovo tidak terkoneksi   negative    negative\n","6489  give option rider wants accept nearer driver t...    neutral    negative\n","4001                                               good   positive    positive"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["#create new df that consist of x_test and y_test for other models to use\n","classifying_df_with_stopwords = X_test.join(y_test)\n","\n","#insert results of naive bayes classifier to df\n","classifying_df_with_stopwords['Naive_Bayes'] = y_pred\n","\n","# Display random 5 rows\n","classifying_df_with_stopwords[['Review_SA_Processed', 'gold_truth', 'Naive_Bayes']].sample(5)\n"]},{"cell_type":"markdown","id":"9f95d126","metadata":{"id":"9f95d126"},"source":["# 3.2 Training a Naive Bayes Classifier (Probablistic) without Stopwords"]},{"cell_type":"code","execution_count":null,"id":"948842b4","metadata":{"id":"948842b4","outputId":"f427f38b-cdf2-4aba-b649-b8b6b85f195f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review_SA_Processed_Removed_Stopwords</th>\n","      <th>Score</th>\n","      <th>gold_truth</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9802</th>\n","      <td>good heavens future taxi trip arrangements tha...</td>\n","      <td>5</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9803</th>\n","      <td>babi customer service nak typing dah ended ses...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>9804</th>\n","      <td>grab app helpful service fast amazing delivere...</td>\n","      <td>5</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>9805</th>\n","      <td>trying book ride broad daylight specific spent...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>9806</th>\n","      <td>cashing easy use credit pay cashless try withd...</td>\n","      <td>1</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  Review_SA_Processed_Removed_Stopwords  Score gold_truth\n","9802  good heavens future taxi trip arrangements tha...      5   positive\n","9803  babi customer service nak typing dah ended ses...      1   negative\n","9804  grab app helpful service fast amazing delivere...      5   positive\n","9805  trying book ride broad daylight specific spent...      1   negative\n","9806  cashing easy use credit pay cashless try withd...      1   negative"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('cleaned_grab_playstore_reviews.csv')\n","\n","df = df.drop('Date', axis=1)\n","\n","#obtain list of polarity based on scores\n","score_polarity = []\n","for i in range(len(df)):\n","    if df['Score'][i] == 3:\n","        score_polarity.append('neutral')\n","    elif df['Score'][i] < 3:\n","        score_polarity.append('negative')\n","    else:\n","        score_polarity.append('positive')\n","\n","# Expand contractions within the reviews which include those like cant, dont\n","def decontract_reviews(text):\n","    return contractions.fix(text)\n","\n","def preprocess_sentence(sentence):\n","    # Convert to lowercase\n","    sentence = sentence.lower()\n","\n","    # Expand contractions\n","    sentence = decontract_reviews(sentence)\n","\n","    # Remove punctuation\n","    sentence = re.sub('[^\\w\\s]', '', sentence)\n","\n","    # Tokenize the sentence\n","    tokens = nltk.word_tokenize(sentence)\n","\n","    # Remove stopwords\n","    filtered_tokens = [token for token in tokens if token not in stop_list]\n","\n","    # Join the filtered tokens using a space character\n","    filtered_sentence = ' '.join(filtered_tokens)\n","\n","    return filtered_sentence\n","\n","df['Review_SA_Processed_Removed_Stopwords'] = df['Review'].apply(preprocess_sentence)\n","df = df.assign(gold_truth=score_polarity)\n","df = df.reindex(columns=['Review_SA_Processed_Removed_Stopwords', 'Score', 'gold_truth'])\n","\n","\n","#clone original df\n","df_copy = df.copy(deep=True)\n","\n","#make x and y for train_test split\n","y = df_copy.pop('gold_truth')\n","\n","X = df_copy\n","\n","df.tail()\n"]},{"cell_type":"code","execution_count":null,"id":"636228c8","metadata":{"id":"636228c8","outputId":"6544b218-cec0-4959-af3c-ef8d5ead39cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished preparing the training data.\n","Finished training the classifier.\n","Finished preparing the test data.\n"]}],"source":["#train-test split the data, where corpus = X and labels = y\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n","\n","# We use the following list to store the sentences, where each sentence itself is a list of words.\n","X_train_corpus = []\n","\n","for i in range(len(X_train)):\n","    # Tokenize the text.\n","    sent = nltk.word_tokenize(X_train['Review_SA_Processed_Removed_Stopwords'].iloc[i])\n","\n","    # Store the sentence into the corpus.\n","    X_train_corpus.append(sent)\n","\n","# Create a dictionary from the corpus.\n","dictionary = gensim.corpora.Dictionary(X_train_corpus)\n","\n","# Store the labeled training data in the following list.\n","labeled_training_data = []\n","\n","# Going through the two lists in parallel to create the labeled data set.\n","for (l, s) in zip(y_train, X_train_corpus):\n","\n","    # Convert the original sentence into a vector.\n","    vector = dictionary.doc2bow(s)\n","\n","    # Create a dict object to store the document vector (in order to use NLTK's classifier later)\n","    sent_as_dict = {id:1 for (id, tf) in vector}\n","\n","    # Add the labeled sentence to the labeled data set.\n","    labeled_training_data.append((sent_as_dict, l))\n","\n","print('Finished preparing the training data.')\n","\n","# Training Naive Bayes classifier.\n","classifierWithoutStopwords = nltk.NaiveBayesClassifier.train(labeled_training_data)\n","\n","print('Finished training the classifier.')\n","\n","# Store the labeled test data in the following list.\n","labeled_test_data = []\n","\n","X_test_corpus = []\n","\n","for i in range(len(X_test)):\n","    # Tokenize the text.\n","    sent = nltk.word_tokenize(X_test['Review_SA_Processed_Removed_Stopwords'].iloc[i])\n","\n","    # Store the sentence into the corpus.\n","    X_test_corpus.append(sent)\n","\n","# Going through the two lists in parallel to create the labeled data set.\n","for (l, s) in zip(y_test, X_test_corpus):\n","\n","    # Convert the original sentence into a vector.\n","    vector = dictionary.doc2bow(s)\n","\n","    # Create a dict object to store the document vector (in order to use NLTK's classifier later)\n","    sent_as_dict = {id:1 for (id, tf) in vector}\n","\n","    # Add the labeled sentence to the labeled data set.\n","    labeled_test_data.append((sent_as_dict, l))\n","\n","print('Finished preparing the test data.')"]},{"cell_type":"code","execution_count":null,"id":"127945cd","metadata":{"id":"127945cd","outputId":"7057e0a5-411e-44d3-d77c-c2d199efef1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Naive Bayes Removed Stopwords\n","Confusion Matrix:\n"," [[ 897  230   27]\n"," [  91   42   16]\n"," [ 292  171 1177]]\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","    negative       0.70      0.78      0.74      1154\n","     neutral       0.09      0.28      0.14       149\n","    positive       0.96      0.72      0.82      1640\n","\n","    accuracy                           0.72      2943\n","   macro avg       0.59      0.59      0.57      2943\n","weighted avg       0.82      0.72      0.75      2943\n","\n"]}],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","\n","# Convert the labeled test data to a list of features and labels\n","test_set = [(features, label) for (features, label) in labeled_test_data]\n","\n","# Create a list of predicted labels and a list of gold truth labels for the test set\n","y_pred = [classifierWithoutStopwords.classify(features) for (features, label) in test_set]\n","y_true = [label for (features, label) in test_set]\n","\n","# Generate confusion matrix\n","nb_conf_mat = confusion_matrix(y_true, y_pred)\n","\n","# Generate classification report\n","nb_class_report = classification_report(y_true, y_pred, output_dict=True)\n","\n","# Confusion Matrix and Classification Report with Stopwords\n","\n","cm_without_stopwords = nb_conf_mat\n","cr_without_stopwords = classification_report(y_true, y_pred)\n","cr_without_stopwords_dict = classification_report(y_true, y_pred, output_dict=True)\n","\n","\n","# Print results\n","print(\"Naive Bayes Removed Stopwords\")\n","print(\"Confusion Matrix:\\n\", cm_without_stopwords)\n","print(\"\\nClassification Report:\\n\", cr_without_stopwords)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ab146301","metadata":{"id":"ab146301","outputId":"63ae46dd-ebc3-4e7b-ae0c-4f58ab751c61"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: Naive Bayes\n","Custom Input |  Predicted Sentiment\n","Good driver service! | positive\n","Bad driver service.. | negative\n","this is an app for driver and rider  | negative\n","i like this app!! | negative\n"]}],"source":["# Define the function to preprocess the sentences\n","def preprocess_sentence(sentence):\n","    # Convert to lowercase\n","    sentence = sentence.lower()\n","\n","    # Expand contractions\n","    sentence = decontract_reviews(sentence)\n","\n","    # Remove punctuation\n","    sentence = re.sub('[^\\w\\s]', '', sentence)\n","\n","    # Tokenize the sentence\n","    tokens = nltk.word_tokenize(sentence)\n","\n","    return tokens\n","\n","# List of sentences to test\n","sentences = [\"Good driver service!\", \"Bad driver service..\", \"this is an app for driver and rider \", \"i like this app!!\"]\n","\n","print(\"Model: Naive Bayes\")\n","print(\"Custom Input\", \"| \", \"Predicted Sentiment\")\n","\n","# Iterate over the list of sentences and predict the sentiment for each\n","for sentence in sentences:\n","    # Preprocess the sentence\n","    processed_sentence = preprocess_sentence(sentence)\n","\n","    # Convert the tokenized sentence to a vector\n","    vector = dictionary.doc2bow(processed_sentence)\n","\n","    # Create a dictionary object to store the document vector (in order to use NLTK's classifier)\n","    sent_as_dict = {id: 1 for (id, tf) in vector}\n","\n","    # Use the Naive Bayes classifier to predict the sentiment of the sentence\n","    predicted_sentiment = classifierWithoutStopwords.classify(sent_as_dict)\n","\n","    # Print the predicted sentiment\n","    print(sentence, \"|\", predicted_sentiment)\n"]},{"cell_type":"code","execution_count":null,"id":"dda2bbaa","metadata":{"scrolled":true,"id":"dda2bbaa","outputId":"8abcc328-166a-46f5-c7a7-bd6334a128a4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review_SA_Processed_Removed_Stopwords</th>\n","      <th>gold_truth</th>\n","      <th>Naive_Bayes_Stopwords_Removed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1285</th>\n","      <td>nice</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>4684</th>\n","      <td>mute notification thangkew</td>\n","      <td>neutral</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4510</th>\n","      <td>hard book</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3438</th>\n","      <td>leave unsafe ride app button cancel booking pr...</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>8406</th>\n","      <td>review still appears restaurants name leaving ...</td>\n","      <td>positive</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  Review_SA_Processed_Removed_Stopwords gold_truth  \\\n","1285                                               nice   positive   \n","4684                         mute notification thangkew    neutral   \n","4510                                          hard book   negative   \n","3438  leave unsafe ride app button cancel booking pr...   negative   \n","8406  review still appears restaurants name leaving ...   positive   \n","\n","     Naive_Bayes_Stopwords_Removed  \n","1285                      positive  \n","4684                      negative  \n","4510                      negative  \n","3438                      negative  \n","8406                      negative  "]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["#create new df that consist of x_test and y_test for other models to use\n","classifying_df_without_stopwords = X_test.join(y_test)\n","\n","#insert results of naive bayes classifier to df\n","classifying_df_without_stopwords['Naive_Bayes_Stopwords_Removed'] = y_pred\n","\n","# Display random 5 rows\n","classifying_df_without_stopwords[['Review_SA_Processed_Removed_Stopwords', 'gold_truth','Naive_Bayes_Stopwords_Removed']].sample(5)"]},{"cell_type":"markdown","id":"70ba7b57","metadata":{"id":"70ba7b57"},"source":["# 4 Comparing NB with/without stopwords removed"]},{"cell_type":"markdown","id":"18ecf829","metadata":{"id":"18ecf829"},"source":["## Compare classification results between the 2 models."]},{"cell_type":"code","execution_count":null,"id":"3a07de4b","metadata":{"id":"3a07de4b","outputId":"e0425c2b-7e86-4938-8dbb-043eb6c4c67c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy_Score</th>\n","      <th>Weighted_Precision_Score</th>\n","      <th>Weighted_Recall_Score</th>\n","      <th>Weighted_F1_Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Naive Bayes with stopwords</td>\n","      <td>0.711519</td>\n","      <td>0.810154</td>\n","      <td>0.711519</td>\n","      <td>0.746591</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Naive Bayes removed stopwords</td>\n","      <td>0.718994</td>\n","      <td>0.817202</td>\n","      <td>0.718994</td>\n","      <td>0.754860</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                           Model  Accuracy_Score  Weighted_Precision_Score  \\\n","0     Naive Bayes with stopwords        0.711519                  0.810154   \n","1  Naive Bayes removed stopwords        0.718994                  0.817202   \n","\n","   Weighted_Recall_Score  Weighted_F1_Score  \n","0               0.711519           0.746591  \n","1               0.718994           0.754860  "]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["#comparing results of Naive Bayes with stopwords, Naive Bayes without stopwords\n","results = {'Model': ['Naive Bayes with stopwords', 'Naive Bayes removed stopwords'],\n","           'Accuracy_Score': [cr_with_stopwords_dict['accuracy'], cr_without_stopwords_dict['accuracy']],\n","           'Weighted_Precision_Score': [cr_with_stopwords_dict['weighted avg']['precision'], cr_without_stopwords_dict['weighted avg']['precision']],\n","           'Weighted_Recall_Score': [cr_with_stopwords_dict['weighted avg']['recall'], cr_without_stopwords_dict['weighted avg']['recall'], ],\n","           'Weighted_F1_Score': [cr_with_stopwords_dict['weighted avg']['f1-score'], cr_without_stopwords_dict['weighted avg']['f1-score'], ],\n","            }\n","\n","\n","results_df = pd.DataFrame(data=results)\n","results_df"]},{"cell_type":"code","execution_count":null,"id":"6b6100ee","metadata":{"id":"6b6100ee","outputId":"0faa796e-75b9-4cdd-e1db-7e824095c836"},"outputs":[{"name":"stdout","output_type":"stream","text":["Highest Accuracy Score: Naive Bayes removed stopwords (0.7189942235813795) \n","Highest Precision Score: Naive Bayes removed stopwords (0.8172017311065883) \n","Highest Recall Score: Naive Bayes removed stopwords (0.7189942235813795) \n","Highest F1 Score: Naive Bayes removed stopwords (0.7548601326220378) \n"]}],"source":["# find index of respective highest score\n","highest_acc_index = results_df.index[results_df['Accuracy_Score'].idxmax()]\n","highest_precision_index = results_df.index[results_df['Weighted_Precision_Score'].idxmax()]\n","highest_recall_index = results_df.index[results_df['Weighted_Recall_Score'].idxmax()]\n","highest_f1_index = results_df.index[results_df['Weighted_F1_Score'].idxmax()]\n","\n","print('Highest Accuracy Score: ' + str(results_df['Model'][highest_acc_index]) + ' (' + str(results_df['Accuracy_Score'][highest_acc_index]) + ') ')\n","print('Highest Precision Score: ' + str(results_df['Model'][highest_precision_index]) + ' (' + str(results_df['Weighted_Precision_Score'][highest_precision_index]) + ') ')\n","print('Highest Recall Score: ' + str(results_df['Model'][highest_recall_index]) + ' (' + str(results_df['Weighted_Recall_Score'][highest_recall_index]) + ') ')\n","print('Highest F1 Score: ' + str(results_df['Model'][highest_f1_index]) + ' (' + str(results_df['Weighted_F1_Score'][highest_f1_index]) + ') ')"]},{"cell_type":"markdown","id":"9ba493a1","metadata":{"id":"9ba493a1"},"source":["## Combine the Dataframes"]},{"cell_type":"code","execution_count":null,"id":"cd346222","metadata":{"id":"cd346222","outputId":"cae471ae-931d-4388-a9da-6b8f60b36ca8"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review_SA_Processed</th>\n","      <th>Review_SA_Processed_Removed_Stopwords</th>\n","      <th>gold_truth</th>\n","      <th>Naive_Bayes</th>\n","      <th>Naive_Bayes_Stopwords_Removed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>put address food delivery grab decided another...</td>\n","      <td>put address food delivery grab decided another...</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>times cancel riders specific reasons</td>\n","      <td>times cancel riders specific reasons</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>never save card details app food orders proces...</td>\n","      <td>never save card details app food orders proces...</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>little application support trouble adding paym...</td>\n","      <td>little application support trouble adding paym...</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>driver lo ga punya etika males ngubungin customer</td>\n","      <td>driver lo ga etika males ngubungin customer</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2938</th>\n","      <td>grab system message cancel booking rebook due ...</td>\n","      <td>grab system message cancel booking rebook due ...</td>\n","      <td>positive</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2939</th>\n","      <td>getting booked need worst experience today</td>\n","      <td>getting booked need worst experience today</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>2940</th>\n","      <td>great service clean drivers friendly courteous</td>\n","      <td>great service clean drivers friendly courteous</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2941</th>\n","      <td>terrible sign experience trying sign using qat...</td>\n","      <td>terrible sign experience trying sign using qat...</td>\n","      <td>negative</td>\n","      <td>neutral</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>2942</th>\n","      <td>good</td>\n","      <td>good</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2943 rows × 5 columns</p>\n","</div>"],"text/plain":["                                    Review_SA_Processed  \\\n","0     put address food delivery grab decided another...   \n","1                  times cancel riders specific reasons   \n","2     never save card details app food orders proces...   \n","3     little application support trouble adding paym...   \n","4     driver lo ga punya etika males ngubungin customer   \n","...                                                 ...   \n","2938  grab system message cancel booking rebook due ...   \n","2939         getting booked need worst experience today   \n","2940     great service clean drivers friendly courteous   \n","2941  terrible sign experience trying sign using qat...   \n","2942                                               good   \n","\n","                  Review_SA_Processed_Removed_Stopwords gold_truth  \\\n","0     put address food delivery grab decided another...   negative   \n","1                  times cancel riders specific reasons   negative   \n","2     never save card details app food orders proces...   negative   \n","3     little application support trouble adding paym...   negative   \n","4           driver lo ga etika males ngubungin customer   negative   \n","...                                                 ...        ...   \n","2938  grab system message cancel booking rebook due ...   positive   \n","2939         getting booked need worst experience today   negative   \n","2940     great service clean drivers friendly courteous   positive   \n","2941  terrible sign experience trying sign using qat...   negative   \n","2942                                               good   positive   \n","\n","     Naive_Bayes Naive_Bayes_Stopwords_Removed  \n","0       negative                      negative  \n","1       negative                      negative  \n","2       negative                      negative  \n","3       negative                      negative  \n","4       negative                      negative  \n","...          ...                           ...  \n","2938    negative                      negative  \n","2939    negative                      negative  \n","2940    positive                      positive  \n","2941     neutral                       neutral  \n","2942    positive                      positive  \n","\n","[2943 rows x 5 columns]"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["classifying_df_without_stopwords = classifying_df_without_stopwords[['Review_SA_Processed_Removed_Stopwords','Naive_Bayes_Stopwords_Removed']]\n","# Concatenate the two dataframes vertically\n","comparison_df = pd.concat([classifying_df_with_stopwords, classifying_df_without_stopwords], axis=1)\n","# comparison_df\n","# Keep only the desired columns\n","comparison_df = comparison_df[[\"Review_SA_Processed\", \"Review_SA_Processed_Removed_Stopwords\", \"gold_truth\",\"Naive_Bayes\", \"Naive_Bayes_Stopwords_Removed\"]]\n","\n","# Reset the index of the resulting dataframe\n","comparison_df = comparison_df.reset_index(drop=True)\n","\n","comparison_df"]},{"cell_type":"markdown","id":"090be021","metadata":{"id":"090be021"},"source":["## Compare wrong outputs with the gold truth"]},{"cell_type":"code","execution_count":null,"id":"16e5e940","metadata":{"scrolled":true,"id":"16e5e940","outputId":"852573d0-8d22-4a28-f58c-a29b76af1943"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Incorrect_Predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Naive Bayes with stopwords</td>\n","      <td>849</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Naive Bayes removed stopwords</td>\n","      <td>827</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                           Model  Incorrect_Predictions\n","0     Naive Bayes with stopwords                    849\n","1  Naive Bayes removed stopwords                    827"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["# Get the Incorrect_Predictions of each model\n","def count_incorrect_predictions(row):\n","    incorrect_count = 0\n","    if row['gold_truth'] != row['Naive_Bayes']:\n","        incorrect_count += 1\n","    if row['gold_truth'] != row['Naive_Bayes_Stopwords_Removed']:\n","        incorrect_count += 1\n","    return incorrect_count\n","\n","comparison_df['incorrect_count'] = comparison_df.apply(count_incorrect_predictions, axis=1)\n","\n","comparison_df['Naive_Bayes_Incorrect'] = comparison_df['gold_truth'] != comparison_df['Naive_Bayes']\n","comparison_df['Naive_Bayes_Stopwords_Removed_Incorrect'] = comparison_df['gold_truth'] != comparison_df['Naive_Bayes_Stopwords_Removed']\n","\n","# Create a new DataFrame to store the results\n","incorrect_predictions_df = pd.DataFrame({\n","    'Model': ['Naive Bayes with stopwords', 'Naive Bayes removed stopwords'],\n","    'Incorrect_Predictions': [\n","        comparison_df['Naive_Bayes_Incorrect'].sum(),\n","        comparison_df['Naive_Bayes_Stopwords_Removed_Incorrect'].sum()\n","    ]\n","})\n","\n","incorrect_predictions_df\n"]},{"cell_type":"code","execution_count":null,"id":"f8e56191","metadata":{"id":"f8e56191","outputId":"66130ee1-859a-4043-e7bf-6ba1249e3fb4"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Accuracy_Score</th>\n","      <th>Weighted_Precision_Score</th>\n","      <th>Weighted_Recall_Score</th>\n","      <th>Weighted_F1_Score</th>\n","      <th>Incorrect_Predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Naive Bayes with stopwords</td>\n","      <td>0.711519</td>\n","      <td>0.810154</td>\n","      <td>0.711519</td>\n","      <td>0.746591</td>\n","      <td>849</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Naive Bayes removed stopwords</td>\n","      <td>0.718994</td>\n","      <td>0.817202</td>\n","      <td>0.718994</td>\n","      <td>0.754860</td>\n","      <td>827</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                           Model  Accuracy_Score  Weighted_Precision_Score  \\\n","0     Naive Bayes with stopwords        0.711519                  0.810154   \n","1  Naive Bayes removed stopwords        0.718994                  0.817202   \n","\n","   Weighted_Recall_Score  Weighted_F1_Score  Incorrect_Predictions  \n","0               0.711519           0.746591                    849  \n","1               0.718994           0.754860                    827  "]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["combined_df = results_df.merge(incorrect_predictions_df, on='Model', how='inner')\n","combined_df"]},{"cell_type":"code","execution_count":null,"id":"8b34455d","metadata":{"id":"8b34455d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"9021f45a","metadata":{"id":"9021f45a"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}